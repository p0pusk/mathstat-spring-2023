\documentclass[./main.tex]{subfiles}


\begin{document}
\section{Теория}

\subsection{Оценки исходной выборки} 

Внешние оценки ищутся как: \\

\begin{equation}
	\underline{\bm{J}} = \underset{1 \leq k \leq n }{min} \underline{\bm{x}}_{k}, \quad \overline{\bm{J}} = \underset{1 \leq k \leq n }{min} \overline{\bm{x}}_{k}
\end{equation}


\subsection{Вычисление моды выборки и максимальной клики}

Имеет смысл распространить понятие моды на обработку интервальных данных, где она будет обозначать интервал тех значений, которые наиболее часты, т. е. встречаются в интервалах обрабатываемых данных наиболее часто. Фактически, это означает, что точки из моды интервальной выборки накрываются наибольшим числом интервалов этой выборки. Ясно, что по самому своему определению понятие моды имеет наибольшее значение (и наибольший смысл) лишь для накрывающих выборок. Иначе, если выборка ненакрывающая, то смысл «частоты» тех или иных значений в пределах рассматриваемых интервалов этой выборки в значительной мере теряется, хотя и не обесценивается. \\
Мода является пересечением интервалов максимальной совместной подвыборки, и если максимальных подвыборок имеется более одной, то мода будет объединением их пересечений, т. е. мультиинтервалом.\\

Алгоритм для нахождения моды интервальной выборки: \\

\begin{enumerate}
	\item $ \bm{I} \longleftarrow \cap_{i=1}^{n} \bm{x}_{i} $
	\item \begin{algorithmic}
		\If {$I \neq  \varnothing $}
		\State mode $\bm{X} \longleftarrow \bm{I}$ ; $\mu \longleftarrow n$
		\Else \\
		Помещаем все концы $\underline{\bm{x}}_{1}, \overline{\bm{x}}_{1}, \ldots, \underline{\bm{x}}_{n}, \overline{\bm{x}}_{n}$ инетрвалов рассматриваемой выборки $\bm{X}$ в один массив $Y = (y_1, y_2, \ldots, y_{2n})$; \\
		Упорядочиваем элементы в $Y$ по возрастанию значений; \\
		Порождаем интервалы $z_i=[y_i,y_{i+1}], i=1,2,\ldots, 2n-1$ (назовем их элементарными подинтервалами измерений); \\
		Для каждого $z_i$ подсчитываем число $\mu_i$ интервалов из выборки $\bm{X}$, включающих интервал $z_i$; \\
		Вычисляем $\mu \longleftarrow \underset {1 \leq i \leq 2n-1}{max} \mu_i$; \\
		Вычисляем номера $k$ интервалов $z_k$, для которых $\mu_k$ равно мксимальному, т.е. $\mu_k = \mu$ и формируем из таких $k$ множество $K = \{k\} \subseteq \{1,2,\ldots, 2n-1\}$; \\
		mode $ \bm{X} \longleftarrow \cup_{k \in K} z_{k}$
		\EndIf
	\end{algorithmic}
\end{enumerate}

Значение максимальной клики равняется: max $\mu_j (\bm{X})$\\

\subsection{Варьирование неопределенности изменений}

Один из приемов выявления достижения совместности выборки интервальных наблюдений основан на представлении о причине несовместности как недооцененной величины неопределенности. Закономерным шагом в этом случае становится поиск некоторой минимальной коррекции величин неопределенности интервальных наблюдений, необходимой для обеспечения совместности задачи построения зависимости. Если величину коррекции каждого интервального наблюдения $\bm{y}_i = [\stackrel{\circ}{y}_i - \epsilon_i, \stackrel{\circ}{y}_i + \epsilon_i]$ выборки $S_n$ выражать коэффициентом его уширения $w_i \geq 1$, а общее изменение выборки характеризовать суммой этих коэффициентов, то минимальная коррекция выборки в виде вектора коэффициентов $w^* = (w_{1}^*, \ldots, w_{n}^{*})$, необходимая для совместности задачи построения $y=f(x, \beta)$ может быть решена решением задачи условной оптимизации: \\

Найти: \\
\begin{equation} \label{eq:2}
	\underset {w, \beta}{\text{min}} \sum\limits_{i=1}^{n} w_i
\end{equation} 

При ограничениях: \\

\begin{equation} \label{eq:3}
	\begin{cases}
		\stackrel{\circ}{y}_i - w_i \epsilon_i \leq f(x_i,\beta) \leq \stackrel{\circ}{y}_i + w_i \epsilon_i, \\
		w_i \geq 1 
	\end{cases}
\end{equation} 

$i = 1, \ldots, n$ \\

Результирующие значения коэффициентов $w_{i}^{*}$, строго превосхожящие единицу, указывают на наблюдения, которые требуют уширения интервалов неопределенности для обеспечения совместности данных и модели. Именно такие наблюдения заслуживают внимания пр ианализе данных на выбросы. Значительное количество подобных наблюдений может говорить либо о неверно выбранной структуре зависимости, либо о том, что величины неопределенности измерений занижены во многих наблюдениях (например, в результате неверной оценки точности измерительного прибора). \\
Следует отметить значительную гибкость языка неравенств. Он даёт возможность переформулировать и расширять систему ограничений для учёта специфики данных и задачи при поиске допустимой коррекции данных, приводящей к разрешению исходной несовместности. Например, если имеются основания считать, что величина неопределённости некоторой группы наблюдений одинакова и при коррекции должна увеличиваться синхронно, то система ограничений может быть пополнена равенствами вида: \\

\begin{equation*}
	w_{i_1} = w_{i_{2}} = \ldots = w_{i_K}, 
\end{equation*}

где $i_1, \ldots, i_K$ -- номера наблюдений группы. В случае, когда в надежности каких-либо наблюдений исследователь уверен полностью, при решении задачи \eqref{eq:2} - \eqref{eq:3} соответствующие им величины $w_i$ можно положить равными единице, т.е. запретить вырьировать их неопределенность. \\
Задачи поиска коэффициентов масштабирования величины неопределенности сформулирована для распространенного случая уравновешенных интервалов погрешности и подразумевает синхронную подвижность верхней и нижней границ интервалов неопределенности измерений $\bm{y}_i$ при сохранении базовых значений интервалов $\stackrel{\circ}{y}_i$ неподвижными. При необходимости, постановка задачи легко обобщается. Например, если интервалы наблюдений не уравновешаны относительно базовых значений, то границы интервальных измерений можно варьировать независимо, масштабируя величины неопределенности $\epsilon_i^{-}$ и $\epsilon_{i}^{+}$ с помощью отделимых коэффициентов $w_i^{-}$ и $w_{i}^{+}$: \\

Найти: \\

\begin{equation}
	\underset {w^{-}, w^{+}, \beta}{\text{min}} \sum\limits_{i=1}^{n} (w_i^{-} + w_i^{+})
\end{equation}

При ограничениях: \\

\begin{equation}
	\begin{cases}
		\stackrel{\circ}{y}_i - w_i^{-} \epsilon_i^{-} \leq f(x_i,\beta) \leq \stackrel{\circ}{y}_i + w_i^{+} \epsilon_i^{+}, \\
		w_i^{-} \geq 1 \\
		w_i^{+} \geq 1 
	\end{cases}
\end{equation}

$i = 1, \ldots, n$ \\

Для линейной по параметрам $\beta$ зависимости $y = f(x, \beta)$ задача представляет собой задачу линейного программирования, для решения которой доступны хорошие и апробированные программы в составе библиотек на различных языках программирования, в виде стандартных процедур систем компьютерной математики, а также в виде интерактивных подсистем электронных таблиц. 

\subsection{Оптимизация по Оскорбину}

Постановка задачи линейного программирования в простейшем виде: \\

Найти: \\

\begin{equation}
	\underset {w, \beta}{\text{min}} w
\end{equation}

При ограничениях: \\

\begin{equation}
	\begin{cases}
		\text{mid} x_i - w \epsilon_i \leq \beta \leq \text{mid} x_i + w \epsilon_i\\
		w \geq 1 
	\end{cases}
\end{equation}

$i = 1, \ldots, n$ \\

\subsection{Индекс Жаккара} 

Для описания выборок, помимо оценок их размеров, желательно иметь дополнительную информацию о мере сходимости элементов выборки. В различных областях анализа данных, биологии, информатике и науках о Земле часто используют различные меры сходства множеств. \\
Рассмотрим один из возможных коэффициентов совместности -- это отношение инфимума по включению к супремуму по включению -- индекс Жаккара: \\

\begin{equation}
	J_i(\bm{X}) = \dfrac{\text{wid}(\wedge_i \bm{x}_i)}{\text{wid} (\vee_i \bm{x}_i)}
\end{equation}

Индекс Жакара непрерывно описывает ситуациии от полной несовместности выборок до полного перекрытия интервалов. Он может принимать значения: 
\begin{equation*}
	-1 \leq J_i(\bm{X}) \leq 1
\end{equation*}

\subsection{Относительная ширина моды}

Относительная ширина моды равна: 

\begin{equation}
	\rho(\text{mode}(\bm{X})) =\dfrac{\text{wid}(\text{mode}(\bm{X}))}{\text{wid} (\vee_i \bm{x}_i)}
\end{equation}

В отличие от минимума по включению, мода выборки всегда является правильным интервалом. В целом получаем: 

\begin{equation*}
	0 \leq \rho(\text{mode}(\bm{X})) \leq 1
\end{equation*}

\newpage
\end{document}
