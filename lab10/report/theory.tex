\section{Теория}

\subsection{Постановка задачи восстановления функциональной зависимости}

Пусть некоторая величина $y$ является функцией от независимых переменных $x_1, \ldots, x_m$:

\begin{equation} \label{eq:get_func_task}
	y = f(\beta, x)
\end{equation}

Где $x = (x_1, \ldots, x_m)$ является вектором независимых переменных, $\beta = (\beta_1, \ldots, \beta_p)$ -- вектор параметров функции. Заметим, что переменные $x_1, \ldots, x_m$ также называются входными, а переменная $y$ -- выходной. \\
\textbf{Задача восстановления функциональной зависимости} заключается в том, чтобы, располагая набором значений $x$ и $y$, найти такие $\beta_1, \ldots, \beta_p$ в выражении \eqref{eq:get_func_task}, которые соответствуют конкретной функции $f$ из параметрического семейства. \\
Если функция $f$ является линейной, то можно записать 
\begin{equation}
	y = \beta_0 + \beta_1 x_1 + \ldots + \beta_m x_m
\end{equation}
В общем случае результаты измерений величин $x_1, \ldots, x_m$ и $y$ являются интервальнозначными 
\begin{equation*}
	\bm{x}_1^{(k)}, \ldots, \bm{x}_m^{(k)}, \bm{y}^{(k)}, k \in \overline{1..n}
\end{equation*} 
$n$ -- число измерений. 

\textbf{Брусом неопределенности $k$ - го измерения функциональной зависимости} будем называть интервальный вектор-брус, образованный интервальными результатами измерений с одинаковыми значениями индекса $k$

\begin{equation}
	(\bm{x}_{k1}, \ldots, \bm{x}_{km}, \bm{y}_k) \subset  \mathbb{R}^{m+1}
\end{equation}

Брус неопределенности измерения является прямым декартовым произведением интервалов неопределенности независимых переменных и зависимой переменной. 

\subsection{Варьирование неопределенности изменений} 

Если величину коррекции каждого интервального наблюдения $\bm{y}_i = [\stackrel{\circ}{y}_i - \epsilon_i, \stackrel{\circ}{y}_i + \epsilon_i]$ выборки $S_n$ выражать коэффициентом его уширения $w_i \geq 1$, а общее изменение выборки характеризовать суммой этих коэффициентов, то минимальная коррекция выборки в виде вектора коэффициентов $w^* = (w_{1}^*, \ldots, w_{n}^{*})$, необходимая для совместности задачи построения $y=f(x, \beta)$ может быть решена решением задачи условной оптимизации: \\

Найти: \\
\begin{equation} \label{eq:lin_opt_task}
	\underset {w, \beta}{\text{min}} \sum\limits_{i=1}^{n} w_i
\end{equation}

При ограничениях: \\

\begin{equation} \label{eq:lin_opt_boundaries}
	\begin{cases}
		\text{mid} \bm{x}_i - w_i \epsilon_i \leq \beta_0 + \beta_i * i \leq \text{mid} \bm{x}_i + w_i \epsilon_i, \\
		w_i \geq 1 
	\end{cases}
\end{equation}

$i \in \overline{1..n}$ \\

Результирующие значения коэффициентов $w_i$, строго превосходящие единицу, указывают на наблюдения, которые требуют уширения интервалов неопределенности для обеспечения совместности данных и модели. \\

\subsection{Варьирование неопределенности изменений с расширением и сужением интервалов} 

Поставим задачу условной оптимизации следующим образом:\\
Найти: \\
\begin{equation} \label{eq:lin_opt_pos_task}
	\underset {w, \beta}{\text{min}} \sum\limits_{i=1}^{n} w_i
\end{equation}
При ограничениях: \\
\begin{equation} \label{eq:lin_opt_pos_boundaries}
	\begin{cases}
		\text{mid} \bm{x}_i - w_i \epsilon_i \leq \beta_0 + \beta_i * i \leq \text{mid} \bm{x}_i + w_i \epsilon_i, \\
		w_i \geq 0 
	\end{cases}
\end{equation}

$i \in \overline{1..n}$ \\ 

Отличие постановки от \eqref{eq:lin_opt_task} и \eqref{eq:lin_opt_boundaries} состоит в том, что интервалы измерений могут как расширяться в случае $w_i \geq 1$, так и сужаться при $0 \leq w_i < 1$ \\

Задавшись каким-то порогом $\alpha : 0<\alpha \leq 1$ можно выделить области входного аргумента $\Psi$, в которых регрессионная зависимость хуже соответствует исходным данным. Например: 
\begin{equation}
	\Psi = \text{arg}_i w_i \geq \alpha
\end{equation}

Для объективного использования этого приема параметр $\alpha$ можно брать, например, из анализа гистограммы распределения веткора $w$. \\
Использование выделения <<подозрительных>> областей даёт основу для других приемов. Например, для построения кусочно-линейной регрессионной зависимости. 

\subsection{Анализ регрессионных остатков} 

В теоретико-вероятностной математической статистике анализ регрессионных остатков один из приемов оценки качества регрессии. \\
Приведем пример пояснения этого приема. <<Если выбранная регрессионная модель хорошо описывает истинную зависимость, то остатки должны быть независимыми, нормально распределенными случайными величинами с нулевым средним, и в значениях должен отсутствовать тренд. Анализ регрессионных остатков -- это процесс проверки выполнения этих условий>> \cite{s:remains}[c. 658-659]. \\
В случае интервальных выборок мы не задаемся вопросом о виде распределения остатков, а будем использовать те возможности, которые появляются при описании объектов и результатов вычислений в виде интервалов. 

\subsection{Информационное множество задачи}

Информационным множеством задачи восстановления функциональной зависимости является множество всех значений параметров функции $\beta$, получаемое в результате обработки интервальных значений входных переменных $x$ и выходной переменной $y$. \\

Один из главных вопросов при построении регрессии -- оценивание её параметров. В зависимости от прикладных целей характер и назначение искомых оценок могут существенно разниться. \\
Внешняя интервальная оценка параметра определяется минимальными и максимальными значениями, которых может достигать значение параметра в информационном множестве. \\
В совокупности интервальные оценки параметров задают брус, описанный вокруг информационного множества и именуемый внешней интервальной оболочкой информационного множества. 


\newpage
